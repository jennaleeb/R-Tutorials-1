<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Tutorial 7 - Logistic Regression &middot; R
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/R-Tutorials-1/public/css/poole.css">
  <link rel="stylesheet" href="/R-Tutorials-1/public/css/syntax.css">
  <link rel="stylesheet" href="/R-Tutorials-1/public/css/hyde.css">
  <link rel="stylesheet" href="/R-Tutorials-1/public/css/custom.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/R-Tutorials-1public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/R-Tutorials-1public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
  
  <!-- Mathjax -->
  <script type="text/javascript"
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    
    <!--Disqus comment counts-->
    <script id="dsq-count-scr" src="//MIE1402.disqus.com/count.js" async></script>
    
</head>


  <body class="theme-base-0b">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/R-Tutorials-1">
          R
        </a>
      </h1>
      <p class="lead">Content for MIE 1402 tutorials</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/R-Tutorials-1">Home</a>

      

      
      
        
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/R-Tutorials-1/about.html">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/R-Tutorials-1/archive.html">Archives</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/R-Tutorials-1/categories.html">Categories</a>
          
        
      
        
          
        
      
      <a class="sidebar-nav-item" href="https://github.com/jennaleeb/R-Tutorials-1">GitHub project</a>
    </nav>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Tutorial 7 - Logistic Regression</h1>
  <span class="post-date">01 Nov 2016</span>
  <h2 id="logistic-regression">Logistic Regression</h2>

<p><strong>Jenna Blumenthal</strong><br />
   Tutorial #: 7<br />
   MIE 1402</p>

<hr />

<h2 id="introduction">Introduction</h2>

<ul>
  <li>Logistic regression is simply regression with an outcome variable that is <strong>categorical</strong></li>
  <li>The predictor variables are continuous or categorical
    <ul>
      <li>if the predictor variables are categorical only, would use a chi-squared test (<em>Field</em>, Ch. 18)</li>
    </ul>
  </li>
  <li>For example, in medical research:
    <ul>
      <li>given a set of cancer patients, what is the likelihood that their tumors are cancerous vs. benign?</li>
      <li>use existing database to establish which variables are influential in predicting malignancy</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="logistic-regression-equation">Logistic regression equation</h2>

<ul>
  <li>We are instead predicting the <strong>probability</strong> of <script type="math/tex">Y</script> occuring, rather than the <strong>value</strong> of <script type="math/tex">Y</script></li>
</ul>

<script type="math/tex; mode=display">P(Y) = \frac{1}{1+e^{-(b_{0}+b_{1}X_{1}+b_{2}X_{2}...)}}</script>

<ul>
  <li>
    <p>Since we cannot apply linear regression when the outcome variable is categorical (would violate the assumption of linearity)</p>
  </li>
  <li>We instead take the logarithm of the equation</li>
  <li>This allows us to express a non-linear relationship in a linear way
    <ul>
      <li><script type="math/tex">P(Y)</script> is continuous, and varies from 0 to 1</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="assessing-the-model">Assessing the model</h2>

<ul>
  <li>for a given occurance, <script type="math/tex">Y</script> will either be 0 (didn’t occur) or 1 (did occur)</li>
  <li>so <script type="math/tex">P(Y)</script> is the <em>chance</em> that it occured</li>
  <li>To assess whether our model fits the data, we can compare the <strong>observed</strong> and <strong>predicted</strong> values of the outcome</li>
  <li>The measure is called the <strong>log-likelihood</strong>
    <ul>
      <li>Analogous to <script type="math/tex">SS_{R}</script> in that it is an indicator of how much unexplained information there is after a model has been fitted</li>
      <li>Large value = lots of unexplained observations</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\text{log-likelihood} = \sum_{i=1}^{N}[Y_{i}ln(P(Y_{i}))+(1-Y_{i})ln(1-P(Y_{i})))]</script>

<hr />

<h2 id="assessing-the-model-1">Assessing the model</h2>

<script type="math/tex; mode=display">\text{log-likelihood} = \sum_{i=1}^{N}[Y_{i}ln(P(Y_{i}))+(1-Y_{i})ln(1-P(Y_{i})))]</script>

<ul>
  <li>recall the <script type="math/tex">ln</script> distribution:</li>
</ul>

<p><img src="/R-Tutorials-1/figure/source/2016-11-01-tutorial-7-logistic-regression/unnamed-chunk-2-1.png" alt="plot of chunk unnamed-chunk-2" /></p>

<ul>
  <li>What happens when<br />
<script type="math/tex">Y_{i} = 0</script>?<br />
<script type="math/tex">Y_{i} = 1</script>?</li>
</ul>

<hr />

<h2 id="deviance-statistic">Deviance statistic</h2>
<ul>
  <li>deviance = <script type="math/tex">-2 x LL</script></li>
  <li>more convenient because it has a chi-square distribution</li>
  <li>easier to calculate significance of value</li>
  <li>We use the deviance to compare models (similar to <strong>aov</strong>(model_1, model_2))
    <ul>
      <li>Does our model improve prediction?</li>
    </ul>

    <p><script type="math/tex">\chi^{2}=2LL(new) - 2LL(baseline)</script><br />
<script type="math/tex">df = k_{new} - k_{baseline}</script></p>
  </li>
  <li>In linear regression, the baseline model is the mean</li>
  <li>In logistic, it is the category with the highest <strong>frequency</strong>
    <ul>
      <li>The category with the higher # of cases (0 or 1) is our “best guess”</li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="other-ways-to-assess-the-model">Other ways to assess the model</h3>
<p>(we won’t go over)
- <script type="math/tex">R</script>, <script type="math/tex">R^{2}</script>
  + Hosmer and Lemeshow’s <script type="math/tex">R^{2}_{L}</script>
  + Cox and Snell’s <script type="math/tex">R^{2}_{CS}</script>
  + Nagelkerke’s <script type="math/tex">R^{2}_{N}</script>
- Each have their own theoretical background and slightly different equations (<em>Field</em>, 317-18)
- AIC, BIC</p>

<hr />

<h2 id="odds-ratio">Odds ratio</h2>

<ul>
  <li>More important to interpreting logistic regression is the value of the odds ratio, <script type="math/tex">e^{B}</script>
    <ul>
      <li>Indicator of the change in odds resulting from a <strong>unit change</strong> in the predictor</li>
    </ul>
  </li>
  <li>The <strong>“odds”</strong> of an event occuring is:</li>
</ul>

<script type="math/tex; mode=display">\text{odds} = \frac{P(event)}{P(no\,event)}</script>

<p>Where:</p>

<script type="math/tex; mode=display">P(event\,Y) = \frac{1}{1+e^{-b_{0}+b_{1}X_{1}+b_{2}X_{2}...}}</script>

<script type="math/tex; mode=display">P(no\,event\,Y) = 1 - P(event\,Y)</script>

<hr />

<h2 id="odds-ratio-1">Odds ratio</h2>

<p>So, the odds <strong>ratio</strong> is</p>

<script type="math/tex; mode=display">\Delta odds = \frac{\text{odds after a unit change in predictor}}{\text{original odds}}</script>

<ul>
  <li>if &gt; 1, as the predictor increases, the odds of the outcome occuring increase</li>
</ul>

<hr />

<h2 id="logistic-regression-in-r">Logistic regression in R</h2>

<p>How does GRE, GPA and prestige of undergrad institution influence admission into graduate school? <a href="http://www.ats.ucla.edu/stat/r/dae/logit.htm">(UCLA Institute for Digital Research and Education)</a></p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"http://www.ats.ucla.edu/stat/data/binary.csv"</span><span class="p">)</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>##   admit gre  gpa rank
## 1     0 380 3.61    3
## 2     1 660 3.67    3
## 3     1 800 4.00    1
## 4     1 640 3.19    4
## 5     0 520 2.93    4
## 6     1 760 3.00    2
</code></pre>
</div>

<hr />

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">mlogit</span><span class="p">)</span><span class="w">

</span><span class="c1">### Let's start by just looking at GPA
</span></code></pre>
</div>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">model.1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glm</span><span class="p">(</span><span class="n">admit</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">gpa</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"binomial"</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">model.1</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>## 
## Call:
## glm(formula = admit ~ gpa, family = "binomial", data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1131  -0.8874  -0.7566   1.3305   1.9824  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -4.3576     1.0353  -4.209 2.57e-05 ***
## gpa           1.0511     0.2989   3.517 0.000437 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 486.97  on 398  degrees of freedom
## AIC: 490.97
## 
## Number of Fisher Scoring iterations: 4
</code></pre>
</div>

<ul>
  <li>The residual deviance is lower than the null deviance (model is now predicting the outcome more accurately than baseline)</li>
  <li>Question of how <em>much</em> better the model predicts the outcome is assessed using the <script type="math/tex">\chi^{2}</script> statistic:
<script type="math/tex">\chi^{2} = 499.98 - 486.97 = 13.01</script></li>
</ul>

<hr />

<p>Or in R:</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">model.1.Chi</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">model.1</span><span class="o">$</span><span class="n">null.deviance</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">model.1</span><span class="o">$</span><span class="n">deviance</span><span class="w">
</span><span class="nf">round</span><span class="p">(</span><span class="n">model.1.Chi</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>## [1] 13.01
</code></pre>
</div>

<p>Find the probability associated with this chi-square statistic by using the pchisq() function, and the difference in degrees of freedom</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">model.1</span><span class="o">$</span><span class="n">df.null</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">model.1</span><span class="o">$</span><span class="n">df.residual</span><span class="w">
</span><span class="n">df</span><span class="w">
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>## [1] 1
</code></pre>
</div>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pchisq</span><span class="p">(</span><span class="n">model.1.Chi</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="p">)</span><span class="w"> </span><span class="c1"># this is the probability we want
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>## [1] 0.0003100148
</code></pre>
</div>

<p>So we can report:<br />
<script type="math/tex">% <![CDATA[
\chi^{2}(1) = 13.01, p < .001 %]]></script></p>

<hr />

<h3 id="assessing-coefficients">Assessing coefficients</h3>

<ul>
  <li>similar to linear regression, the <script type="math/tex">b</script> estimate replaces the value in our original equation</li>
  <li>represents the change in the logit of the outcome variable associated with a one-unit change in the predictor variable
    <ul>
      <li>the natural logarithm (<script type="math/tex">ln</script>) if the odds of <script type="math/tex">Y</script> occuring</li>
    </ul>
  </li>
  <li><script type="math/tex">z</script>-statistic tells us whether the <script type="math/tex">b</script> coefficient is significantly different from 0</li>
</ul>

<hr />

<h3 id="odds-ratio-2">Odds ratio</h3>

<script type="math/tex; mode=display">\frac{\text{odds after a unit change in the predictor}}{\text{original odds}}</script>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="nf">exp</span><span class="p">(</span><span class="n">model.1</span><span class="o">$</span><span class="n">coefficients</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>## (Intercept)         gpa 
##  0.01280926  2.86082123
</code></pre>
</div>

<hr />

<h2 id="more-than-one-predictor">More than one predictor</h2>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">str</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>## 'data.frame':	400 obs. of  4 variables:
##  $ admit: int  0 1 1 1 0 1 1 0 1 0 ...
##  $ gre  : int  380 660 800 640 520 760 560 400 540 700 ...
##  $ gpa  : num  3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ...
##  $ rank : int  3 3 1 4 4 2 1 2 3 2 ...
</code></pre>
</div>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">data</span><span class="o">$</span><span class="n">rank</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">factor</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">rank</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<hr />

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">model.2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glm</span><span class="p">(</span><span class="n">admit</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">gpa</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">gre</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">rank</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"binomial"</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">model.2</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>## 
## Call:
## glm(formula = admit ~ gpa + gre + rank, family = "binomial", 
##     data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6268  -0.8662  -0.6388   1.1490   2.0790  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.989979   1.139951  -3.500 0.000465 ***
## gpa          0.804038   0.331819   2.423 0.015388 *  
## gre          0.002264   0.001094   2.070 0.038465 *  
## rank2       -0.675443   0.316490  -2.134 0.032829 *  
## rank3       -1.340204   0.345306  -3.881 0.000104 ***
## rank4       -1.551464   0.417832  -3.713 0.000205 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 458.52  on 394  degrees of freedom
## AIC: 470.52
## 
## Number of Fisher Scoring iterations: 4
</code></pre>
</div>

<hr />

<h3 id="interpretation">Interpretation</h3>

<ul>
  <li>for every unit change in <strong>gre</strong> and <strong>gpa</strong>, we get the value that the log odds value increases by</li>
  <li>But for interpreting <strong>rank</strong> recall:</li>
</ul>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">contrasts</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">rank</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>##   2 3 4
## 1 0 0 0
## 2 1 0 0
## 3 0 1 0
## 4 0 0 1
</code></pre>
</div>
<ul>
  <li>For <strong>rank</strong>, we are getting the change in log odds that occurs when you compare institution with rank 2 versus rank 1</li>
</ul>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="nf">exp</span><span class="p">(</span><span class="n">model.2</span><span class="o">$</span><span class="n">coefficients</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>## (Intercept)         gpa         gre       rank2       rank3       rank4 
##   0.0185001   2.2345448   1.0022670   0.5089310   0.2617923   0.2119375
</code></pre>
</div>
<ul>
  <li>Now we can say that for a one unit increase in gpa, the odds of being admitted to graduate school (versus not being admitted) increase by a factor of 2.23</li>
</ul>

<hr />

<h2 id="comparing-models">Comparing models</h2>

<ul>
  <li>You can use the difference in deviance equation</li>
  <li>Or in R, simply:</li>
</ul>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">anova</span><span class="p">(</span><span class="n">model.1</span><span class="p">,</span><span class="w"> </span><span class="n">model.2</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>## Analysis of Deviance Table
## 
## Model 1: admit ~ gpa
## Model 2: admit ~ gpa + gre + rank
##   Resid. Df Resid. Dev Df Deviance
## 1       398     486.97            
## 2       394     458.52  4    28.45
</code></pre>
</div>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pchisq</span><span class="p">(</span><span class="m">28.45</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>## [1] 1.010924e-05
</code></pre>
</div>

<hr />

<h2 id="references">References</h2>

<p>Field A, Miles J, Field Z. Discovering Statistics Using R. London: Sage; 2012.</p>

<p>R Data Analysis Examples: Logit Regression. UCLA Institute for Digital Research and Education. http://www.ats.ucla.edu/stat/r/dae/logit.htm</p>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/11/07/tutorial-8-ancova.html">
            Tutorial 8 - ANCOVA
            <small>07 Nov 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/10/25/tutorial-6-anova.html">
            Tutorial 6 - Comparing several means (ANOVA)
            <small>25 Oct 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/10/11/tutorial-5-data-manipulation-reporting.html">
            Tutorial 5 - Data manipulation and reporting
            <small>11 Oct 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>







<div id="disqus_thread"></div>

<script>(function() {
        var d = document, s = d.createElement('script');
        s.src = '//MIE1402.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


    </div>

  </body>
</html>

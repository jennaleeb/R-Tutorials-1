---
title       : Tutorial 7 - Logistic Regression
subtitle    : 
author      : Jenna Blumenthal
job         : MIE 1402
framework   : deckjs        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]     # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---

```{r include=FALSE}
library(knitr)
library(ggplot2)
```

## Logistic Regression

   **Jenna Blumenthal**  
   Tutorial #: 7  
   MIE 1402

---

## Introduction

- Logistic regression is simply regression with an outcome variable that is **categorical**
- The predictor variables are continuous or categorical
  + if the predictor variables are categorical only, would use a chi-squared test (_Field_, Ch. 18)
- For example, in medical research:
  + given a set of cancer patients, what is the likelihood that their tumors are cancerous vs. benign?
  + use existing database to establish which variables are influential in predicting malignancy

---

## Logistic regression equation

- We are instead predicting the **probability** of $Y$ occuring, rather than the **value** of $Y$

$$ P(Y) = \frac{1}{1+e^{-(b_{0}+b_{1}X_{1}+b_{2}X_{2}...)}} $$

- Since we cannot apply linear regression when the outcome variable is categorical (would violate the assumption of linearity)
  
- We instead take the logarithm of the equation
- This allows us to express a non-linear relationship in a linear way
  + $P(Y)$ is continuous, and varies from 0 to 1

---

## Assessing the model

- for a given occurance, $Y$ will either be 0 (didn't occur) or 1 (did occur)
- so $P(Y)$ is the _chance_ that it occured
- To assess whether our model fits the data, we can compare the **observed** and **predicted** values of the outcome
- The measure is called the **log-likelihood**
  + Analogous to $SS_{R}$ in that it is an indicator of how much unexplained information there is after a model has been fitted
  + Large value = lots of unexplained observations

$$\text{log-likelihood} = \sum_{i=1}^{N}[Y_{i}ln(P(Y_{i}))+(1-Y_{i})ln(1-P(Y_{i})))]$$

--- &two-col

## Assessing the model

$$\text{log-likelihood} = \sum_{i=1}^{N}[Y_{i}ln(P(Y_{i}))+(1-Y_{i})ln(1-P(Y_{i})))]$$

*** {name: left}
- recall the $ln$ distribution:

```{r, echo=FALSE}
x <- seq(0, 1, length.out = 100)
y <- log(x)
data <- data.frame(x,y)
ggplot(data = data, aes(x,y)) + geom_point() + labs(x="X", y="ln(X)")
```

*** {name: right}

- What happens when  
  $Y_{i} = 0$?  
  $Y_{i} = 1$?

---

## Deviance statistic
- deviance = $-2 x LL$
- more convenient because it has a chi-square distribution
- easier to calculate significance of value
- We use the deviance to compare models (similar to **aov**(model_1, model_2))
  + Does our model improve prediction?  
  
$$\chi^{2}=2LL(new) - 2LL(baseline)$$  
$$df = k_{new} - k_{baseline}$$
- In linear regression, the baseline model is the mean
- In logistic, it is the category with the highest **frequency**
  + The category with the higher # of cases (0 or 1) is our "best guess"

---

### Other ways to assess the model
(we won't go over)
- $R$, $R^{2}$
  + Hosmer and Lemeshow's $R^{2}_{L}$
  + Cox and Snell's $R^{2}_{CS}$
  + Nagelkerke's $R^{2}_{N}$
- Each have their own theoretical background and slightly different equations (_Field_, 317-18)
- AIC, BIC

---

## Odds ratio

- More important to interpreting logistic regression is the value of the odds ratio, $e^{B}$
  + Indicator of the change in odds resulting from a **unit change** in the predictor
  
- The **"odds"** of an event occuring is:  

$$\text{odds} = \frac{P(event)}{P(no\,event)}$$

Where:  

$P(event\,Y) = \frac{1}{1+e^{-b_{0}+b_{1}X_{1}+b_{2}X_{2}...}}$

$P(no\,event\,Y) = 1 - P(event\,Y)$

---

## Odds ratio

So, the odds **ratio** is

$$\Delta odds = \frac{\text{odds after a unit change in predictor}}{\text{original odds}}$$

- if > 1, as the predictor increases, the odds of the outcome occuring increase

---

## Logistic regression in R

How does GRE, GPA and prestige of undergrad institution influence admission into graduate school? [(UCLA Institute for Digital Research and Education)](http://www.ats.ucla.edu/stat/r/dae/logit.htm)
```{r}
data <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
head(data)
```

---

```{r, warning=FALSE, message=FALSE}
library(mlogit)

### Let's start by just looking at GPA
```{r}
model.1 <- glm(admit ~ gpa, data = data, family = "binomial")
summary(model.1)
```

- The residual deviance is lower than the null deviance (model is now predicting the outcome more accurately than baseline)
- Question of how _much_ better the model predicts the outcome is assessed using the $\chi^{2}$ statistic:
$$\chi^{2} = 499.98 - 486.97 = 13.01$$

---

Or in R:

```{r}
model.1.Chi <- model.1$null.deviance - model.1$deviance
round(model.1.Chi, 2)
```

Find the probability associated with this chi-square statistic by using the pchisq() function, and the difference in degrees of freedom
```{r}
df <- model.1$df.null - model.1$df.residual
df
1 - pchisq(model.1.Chi, df) # this is the probability we want
```

So we can report:  
$$\chi^{2}(1) = 13.01, p < .001 $$

---

### Assessing coefficients

- similar to linear regression, the $b$ estimate replaces the value in our original equation
- represents the change in the logit of the outcome variable associated with a one-unit change in the predictor variable
  + the natural logarithm ($ln$) if the odds of $Y$ occuring
- $z$-statistic tells us whether the $b$ coefficient is significantly different from 0

---

### Odds ratio

$$\frac{\text{odds after a unit change in the predictor}}{\text{original odds}}$$

```{r}
exp(model.1$coefficients)
```

---

## More than one predictor

```{r}
str(data)
data$rank <- factor(data$rank)
```

---
```{r}
model.2 <- glm(admit ~ gpa + gre + rank, data = data, family = "binomial")
summary(model.2)
```

---

### Interpretation

- for every unit change in **gre** and **gpa**, we get the value that the log odds value increases by
- But for interpreting **rank** recall:
```{r}
contrasts(data$rank)
```
- For **rank**, we are getting the change in log odds that occurs when you compare institution with rank 2 versus rank 1

```{r}
exp(model.2$coefficients)
```
- Now we can say that for a one unit increase in gpa, the odds of being admitted to graduate school (versus not being admitted) increase by a factor of 2.23

---

## Comparing models

- You can use the difference in deviance equation
- Or in R, simply:

```{r}
anova(model.1, model.2)
1 - pchisq(28.45, 4)
```

---

## References

Field A, Miles J, Field Z. Discovering Statistics Using R. London: Sage; 2012.

R Data Analysis Examples: Logit Regression. UCLA Institute for Digital Research and Education. http://www.ats.ucla.edu/stat/r/dae/logit.htm

---
title       : Tutorial 10 - Non-parametric tests and categorical data
subtitle    : 
author      : Jenna Blumenthal
job         : MIE 1402
framework   : deckjs        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]     # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---

```{r include=FALSE}
library(knitr)
library(ggplot2)
```

## Non-parametric tests and categorical data

   **Jenna Blumenthal**  
   Tutorial #: 10  
   MIE 1402

---

## Introduction

- Up until now, we've looked at various approaches to look for differences between means
- But these rely on the parametric assumptions
- Not always possible to correct for using transformations, segmentation, etc.
- Instead, use tests that don't rely on these assumptions
- Most of these work by **ranking** the data

---

## General 'analogies'
<table style="text-align: left;"></table>

<style type="text/css">
table { text-align: left}
.deck-container td {border: 3px solid #95a5a6; padding: 7px}
</style>

| Parametric              	| Non-parametric                         	|
|-------------------------	|----------------------------------------	|
| Independent t-test      	| Wilcoxon rank-sum                      	|
| Dependent t-test        	| Wilcoxon signed-rank                   	|
| Independent ANOVA       	| Kruskal-Wallis / Jonckheere-Terpstra    |
| Repeated-measures ANOVA 	| Friedman's ANOVA                       	|

---

## Theory of the Wilcoxon rank-sum test

- rank the scores, ignoring which group they originated
- if there is _no_ difference between groups, you would assume there would be some ranked high, some ranked low, in both groups
  - sum of ranks: group 1 $\approx$ group 2
  
- once we've ranked the data, sum up the ranks in each group
- correct for the # of people in each group (otherwise big group = large sum of ranks)
  mean rank = $\frac{N(N+1)}{2}$
- $W$ = sum of ranks - mean rank

---

### Significance and effect size

- Take the smaller of $W_{1}$, $W_{2}$ 
- **R** calculates the associate $p$ value
  - either by Monte Carlo simulation
  - or if $N>40$, **R** assumes the distribution of the $W$ statistic is normal
    - then can also calculate effect size using $r = \frac{z}{\sqrt{N}}$


---

## Wilcoxon signed-rank test

- Scores come from the same entity
- Test the assumption of normality of the differences between the scores (Shapiro-Wilk etc)
- Calculate difference between scores
- Add up all the (+)'s and all the (-)'s
- The smaller values is used as the test statistic $T$
- Again correct for group size using the mean ($\bar{T}$)
- Find $z$ score and $p$ value using
  $z = \frac{T - \bar{T}}{SE_{\bar{T}}}$

---

## Comparing two groups in R

Both independent (rank-sum) and dependent (signed-rank) Wilcoxon can be run using:

```{r eval=FALSE}
model <- wilcox.test(group1, group2, paired = TRUE/FALSE, correct = FALSE)
```

---

## Kruskal-Wallis test

- comparing multiple independent groups
- However, recall that the $F$ statistic can be robust to violations of assumptions
  - see Sec. 10.3 and Jane Superbrain 10.2 (pg 412-414)
- can also use this non-parametric approach
- Test statistic derived from ranking the data regardless of group, adding up the ranks in each group
- Similarly, the Jonckheere-Terpstra statistic tests for an ordered pattern (trend) in the groups you are comparing

---

## Friedman's ANOVA

- Several conditions, same entities
- Again, test statistic derived from ranking (but this time each group is ranked, within-participant)
- Ranks added up, produces test statistic

---

## R example

Grab data: [http://bit.ly/2flElhq](http://bit.ly/2flElhq)

```{r}
data <- read.delim("coulrophobia.dat")
str(data)
```

---

## Testing assumptions

```{r}
by(data$beliefs, data$infotype, shapiro.test)
```

---

## Testing assumptions

```{r}
library(car)
leveneTest(data$beliefs ~ data$infotype)
```

---

## Running the K-W test in R

```{r}
clown.model.1 <- kruskal.test(beliefs ~ infotype, data = data)
clown.model.1

library(pgirmess)
kruskalmc(beliefs ~ infotype, data = data)
```

---

### More focused comparisons

```{r}
contrasts(data$infotype)
levels(data$infotype) <- c("None", "Advert", "Story", "Exposure") # re-order levels
kruskalmc(beliefs ~ infotype, data = data, cont = "two-tailed") # compare all to first level
```

---

## Categorical data

- So far we've mostly looked at predicting continuous variables (except logistic regression)
- For categorical, we instead look at frequencies rather than the mean
- Consider a simple contingency table, where we analyze the # of 'things' that fall into each combination of categories

---

## Pearson Chi-Square test

- Test for relationship between 2 categorical variables
- Compare the frequencies you observe vs. the frequencies you would expect to see by chance

  $\chi^{2} = \frac{\sum (observed_{ij} - model_{ij})^{2}}{model_{ij}}$ (where $i,j$ are rows/column numbers of contingency table)
  
- So what is the 'model'?
  - In ANOVA: group means
  - Here: expected frequency $model_{ij} = \frac{row-total_{i} \times column-total_{j}}{total -observations}$
  - So in a 2 x 2 table, each cell would have a $model_{ij}$

---

## Pearson Chi-Square test

- once $\chi^{2}$ is calculated, we can find the $dof = (r-1)(c-1)$
- So for 2 x 2 table, dof = 1, then look at the distribution

### Assumptions:
- check to make sure no expected frequencies are < 5
- independence: can't use $\chi^{2}$ test on a repeated-measures design

- Break down significant test w/ standardized residuals
  - analogous to breaking down a significant ANOVA

  $standardized\,residual = \frac{observed_{ij} - model_{ij}}{\sqrt{model_{ij}}}$
  - behaves like a z-score, can assess signficance directly

---

## Categorical analysis in R

Grab data: [http://bit.ly/2fZwh7r](http://bit.ly/2fZwh7r)

```{r}
bike_data <- read.delim("Handlebars.dat")
bike_data

```

---

```{r}
# Create contingency table
one_hand <- subset(bike_data$Frequency, bike_data$Hands == "One Handed")
two_hand <- subset(bike_data$Frequency, bike_data$Hands == "Two Handed")
bike_table <- rbind(one_hand, two_hand)
bike_table
colnames(bike_table) <- c("Dutch", "English")
```

---

```{r eval=FALSE}
library(gmodels)
CrossTable(bike_table, fisher = FALSE, chisq = TRUE, expected = TRUE, sresid = TRUE)
```

---

```{r echo=FALSE}
CrossTable(bike_table, fisher = FALSE, chisq = TRUE, expected = TRUE, sresid = TRUE)
```

---

## Effect size

- most useful measures is the odds ratio

$\textrm{odds}_{\textrm{Dutch bike one-handed}} = \frac{\textrm{number that bike one-handed}}{\textrm{number that bike two-handed}}$

$\textrm{odds}_{\textrm{English bike one-handed}} = \frac{\textrm{number that bike one-handed}}{\textrm{number that bike two-handed}}$

$\textrm{odds ratio} = \frac{\textrm{odds}_{\textrm{Dutch bike one-handed}}}{\textrm{odds}_{\textrm{English bike one-handed}}}$

---

## References

Field A, Miles J, Field Z. Discovering Statistics Using R. London: Sage; 2012.